{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"eecs442-project-code.ipynb","provenance":[{"file_id":"14tOHIbAmawYa0Ywq9o5s01iTRBzTSsXW","timestamp":1587271048680}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HtqfhfcSMHN3","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","import torch\n","drive.mount('/content/gdrive', force_remount=True)\n","print(torch.cuda.is_available())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vWoQaepRMRIZ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm # Displays a progress bar\n","import os\n","import torch\n","from sklearn.model_selection import train_test_split\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import Dataset, Subset, DataLoader, random_split, sampler\n","from torch.optim import lr_scheduler\n","from PIL import Image\n","import time\n","import copy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yskEL46FYXwl","colab_type":"code","colab":{}},"source":["ls '/content/gdrive/Shared drives/eecs442-project/arc10'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CiQV51kxECHD","colab_type":"code","colab":{}},"source":["ls '/content/gdrive/Shared drives/eecs442-project/arc25'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ki3zQgddDbxF","colab_type":"text"},"source":["# **Load Data for ResNet**"]},{"cell_type":"code","metadata":{"id":"j-Ro-Wetus68","colab_type":"code","colab":{}},"source":["def LoadTrainDataSet(train_dir, batch_size, \n","                     train_transform, valid_transform, validation_size=0.2):\n","    print(\"Loading Train Data ...\")\n","    \n","    train_data = datasets.ImageFolder(root=train_dir, transform=train_transform)\n","    valid_data = datasets.ImageFolder(root=train_dir, transform=valid_transform)\n","\n","    mapping = train_data.class_to_idx\n","\n","    # Creating data indices for train and validation splits:\n","    data_size = len(train_data)\n","    indices = list(range(data_size))\n","    split = int(np.floor(validation_size * data_size))\n","\n","    targets = train_data.targets\n","    train_idx, valid_idx = train_test_split(np.arange(len(targets)), \n","                                            test_size=validation_size, \n","                                            random_state=42, \n","                                            shuffle=True, stratify=targets)\n","\n","    train_sampler = sampler.SubsetRandomSampler(train_idx)\n","    valid_sampler = sampler.SubsetRandomSampler(valid_idx)\n","\n","    train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","    validation_loader = DataLoader(valid_data, batch_size=batch_size, sampler=valid_sampler)\n","\n","    print(\"Number of classes loaded: \", len(train_data.class_to_idx))\n","    print(\"Classes loaded: \", train_data.classes)\n","\n","    print(\"Number of images loaded in training dataset: \", len(train_sampler))\n","    print(\"Number of images loaded in validation dataset: \", len(valid_sampler))\n","  \n","    print(\"Number of batches in training loader: \", len(train_loader))\n","    print(\"Number of batches in valuation loader: \", len(validation_loader))\n","\n","    print(\"Finished Loading Train Data\")\n","    print()\n","\n","    return train_loader, validation_loader, len(train_sampler), len(valid_sampler), mapping\n","\n","def LoadTestDataSet(test_dir, batch_size, test_transform):\n","    print(\"Loading Test Data ...\")\n","\n","    data = datasets.ImageFolder(root=test_dir, transform=test_transform)\n","    data_loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n","    \n","    print(\"Number of classes loaded: \", len(data.classes))\n","    print(\"Classes loaded: \", data.classes)\n","    print(\"Number of images loaded: \", len(data))\n","    print(\"Finished Loading Test Data\")\n","    print()\n","\n","    return data_loader, data.classes\n","\n","\n","# Image transformations\n","trans = {\n","    # Train uses data augmentation\n","    'train':\n","    transforms.Compose([\n","        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n","        transforms.RandomRotation(degrees=15),\n","        transforms.ColorJitter(),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.CenterCrop(size=224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    # Validation does not use augmentation\n","    'valid':\n","    transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], \n","                             [0.229, 0.224, 0.225])\n","    ]),\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZxfspxY-MSrY","colab_type":"code","colab":{}},"source":["trainloader, valloader = None, None\n","train_size, val_size = 0, 0\n","testloader = None\n","\n","num_epoch = 10\n","num_classes = 25\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Configure device\n","print(device)\n","\n","dir = '/content/gdrive/Shared drives/eecs442-project/'\n","if num_classes == 10:\n","    train_dir = dir + 'arc10/arc10_train'\n","    test_dir = dir + '/arc10/arc10_test'\n","else: \n","    train_dir = dir + 'arc25/arc25_train'\n","    test_dir = dir + 'arc25/arc25_test'\n","\n","trainloader, valloader, train_size, val_size, mapping = LoadTrainDataSet(train_dir, 32, trans['train'], trans['valid'])\n","testloader, class_names = LoadTestDataSet(test_dir, 8, trans['valid'])\n","\n","dataloaders = {'train': trainloader, 'val': valloader}\n","dataset_sizes = {'train':  train_size, 'val': val_size}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"irKCI6Ck2TzR","colab_type":"code","colab":{}},"source":["classes_train_data = {\n","  'Achaemenid architecture': 55,\n","  'American Foursquare architecture': 47,\n","  'American craftsman style': 156,\n","  'Ancient Egyptian architecture': 205,\n","  'Art Deco architecture': 293,\n","  'Art Nouveau architecture': 360,\n","  'Baroque architecture': 191,\n","  'Bauhaus architecture': 74,\n","  'Beaux-Arts architecture': 153,\n","  'Byzantine architecture': 89,\n","  'Chicago school architecture': 122,\n","  'Colonial architecture': 142,\n","  'Deconstructivism': 170,\n","  'Edwardian architecture': 63,\n","  'Georgian architecture': 123,\n","  'Gothic architecture': 87,\n","  'Greek Revival architecture': 262,\n","  'International style': 166,\n","  'Novelty architecture': 170,\n","  'Palladian architecture': 90,\n","  'Postmodern architecture': 130,\n","  'Queen Anne architecture': 340,\n","  'Romanesque architecture': 86,\n","  'Russian Revival architecture': 132,\n","  'Tudor Revival architecture': 130\n","}\n","\n","labels = torch.zeros(25, dtype=torch.float)\n","for arcclass, num in classes_train_data.items(): \n","  label = mapping[arcclass]\n","  labels[label] = num\n","\n","weight = 1.0 / labels\n","weight = weight / torch.sum(weight)\n","weight = weight.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NtVRBXyw21f-","colab_type":"text"},"source":["# **Class distribution in data sets**"]},{"cell_type":"code","metadata":{"id":"oqanmEV83GpT","colab_type":"code","colab":{}},"source":["idx2class_val = {v: 0 for _, v in mapping.items()}\n","for _, labels in valloader:\n","  for label in labels:\n","    idx2class_val[label.item()] += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLoeUc_acSV8","colab_type":"code","colab":{}},"source":["before = [13, 10, 23, 46, 67, 64, 42, 14, 28, 13, 27, 21, 35, 12, 28, 11, 63, 39, 32, 16, 26, 73, 20, 23, 21]\n","after = [11, 9, 31, 41, 59, 72, 38, 15, 31, 18, 24, 29, 34, 13, 25, 17, 53, 33, 34, 18, 26, 68, 17, 26, 26]\n","original = [69,59,195,256,366,450,239,92,191,111,153,177,213,79,154, 109,327,207,212,113,163,425,107,165,162]\n","\n","scaled = [int((768/4794)*val) for val in original]\n","print(scaled)\n","\n","classes = [name[:-12] for name in class_names]\n","\n","barWidth = 0.25\n","# Set position of bar on X axis\n","width = 0.35  # the width of the bars\n","x = np.arange(len(classes)) \n","\n","# Make the plot\n","plt.style.use('default')\n","fig, ax = plt.subplots(figsize=(10, 6))\n","plt.bar(x+0, before, width, color='tab:blue', label='Uniform Random Sampling')\n","plt.bar(x+0.25, after, width, color='tab:orange', label='Stratified Sampling')\n","plt.bar(x+0.5, scaled, width, color='tab:green', label='Actual Dataset, Scaled')\n"," \n","# Add xticks on the middle of the group bars\n","ax.set_ylabel('Number of Images')\n","ax.set_title('Class Distribution in Validation Set Before and After Stratified Sampling ')\n","ax.set_xticks(x+0.25)\n","ax.set_xticklabels(classes)\n","ax.legend()\n","plt.xticks(rotation='vertical')\n","fig.tight_layout()\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qDAC05JU1X9D","colab_type":"text"},"source":["# **Training and Evaluation Functions**"]},{"cell_type":"code","metadata":{"id":"rXKNf7HPV3_g","colab_type":"code","colab":{}},"source":["def train(model, trloader, valloader, criterion, optimizer, num_epoch = 10): # Train the model\n","    print(\"Start training...\")\n","    model.train() # Set the model to training mode\n","    trloss = []\n","    valloss = []\n","    for i in range(num_epoch):\n","        running_loss = 0.0\n","        running_corrects = 0\n","        for batch, label in tqdm(trloader):\n","            batch = batch.to(device)\n","            label = label.to(device)\n","            optimizer.zero_grad() # Clear gradients from the previous iteration\n","            pred = model(batch) # This will call Network.forward() that you implement\n","            _, preds = torch.max(pred , 1)\n","            loss = criterion(pred, label) # Calculate the loss\n","\n","            # statistics\n","            running_loss += loss.item() * batch.size(0)\n","            running_corrects += torch.sum(preds == label.data)\n","\n","            loss.backward() # Backprop gradients to all tensors in the network\n","            optimizer.step() # Update trainable weights\n","        \n","        epoch_loss = running_loss / train_size\n","        epoch_acc = running_corrects.double() / train_size\n","        trloss.append(np.mean(epoch_loss))\n","        print('Epoch {}, Train Loss: {:.4f} Acc: {:.4f}'.format(i+1, epoch_loss, epoch_acc)) \n","        \n","        running_loss = 0.0\n","        running_corrects = 0\n","        for batch, label in tqdm(valloader):\n","            batch = batch.to(device)\n","            label = label.to(device)\n","            with torch.no_grad():\n","                pred = model(batch) # This will call Network.forward() that you implement\n","                _, preds = torch.max(pred , 1)\n","                loss = criterion(pred, label) # Calculate the loss\n","                # statistics\n","                running_loss += loss.item() * batch.size(0)\n","                running_corrects += torch.sum(preds == label.data)\n","\n","        epoch_loss = running_loss / val_size\n","        epoch_acc = running_corrects.double() / val_size\n","        valloss.append(np.mean(epoch_loss)) \n","        print('Epoch {}, Val Loss: {:.4f} Acc: {:.4f}'.format(i+1, epoch_loss, epoch_acc))      \n","\n","    epochs = np.arange(0, num_epoch, 1)\n","    plt.style.use('default')\n","    plt.plot(epochs, trloss, 'b', label='Training loss')\n","    plt.plot(epochs, valloss, 'r', label='Validation loss')\n","    plt.ylabel('Classification loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loc='upper right', shadow=False, ncol=2)\n","    plt.show()\n","        \n","    print(\"Done!\")\n","\n","def evaluate(model, loader): # Evaluate accuracy on validation / test set\n","    model.eval() # Set the model to evaluation mode\n","    correct = 0\n","    with torch.no_grad(): # Do not calculate grident to speed up computation\n","        for batch, label in tqdm(loader):\n","            batch = batch.to(device)\n","            label = label.to(device)\n","            pred = model(batch)\n","            correct += (torch.argmax(pred,dim=1)==label).sum().item()\n","    acc = correct/len(loader.dataset)\n","    print(\"Evaluation accuracy: {}\".format(acc))\n","    return acc\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gB45ihH613SL","colab_type":"code","colab":{}},"source":["# this train_model function has been adapted from\n","# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n","# It accounts for learning rate decay scheduler that previous train funciton didn't\n","\n","def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    trloss = []\n","    valloss = []\n","    tracc = []\n","    valacc = []\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","  \n","            if phase == 'train':\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))\n","\n","            if phase == 'train':\n","              trloss.append(epoch_loss)\n","              tracc.append(epoch_acc)\n","            else:\n","              valloss.append(epoch_loss)\n","              valacc.append(epoch_acc)\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    epochs = np.arange(0, num_epochs, 1)\n","    plt.style.use('default')\n","    plt.subplot(2, 1, 1)\n","    plt.plot(epochs, trloss, 'b', label='Training loss')\n","    plt.plot(epochs, valloss, 'r', label='Validation loss')\n","    plt.ylabel('Classification loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loc='upper right', shadow=False, ncol=2)\n","\n","    plt.subplot(2, 1, 2)\n","    plt.plot(epochs, tracc, 'b', label='Training accuracy')\n","    plt.plot(epochs, valacc, 'r', label='Validation accuracy')\n","    plt.ylabel('Classification accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(loc='lower right', shadow=False, ncol=2)\n","\n","    plt.show()\n","        \n","    print(\"Done!\")\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model\n","\n","def evaluate(model, loader): # Evaluate accuracy on validation / test set\n","    model.eval() # Set the model to evaluation mode\n","    correct = 0\n","    with torch.no_grad(): # Do not calculate grident to speed up computation\n","        for batch, label in tqdm(loader):\n","            batch = batch.to(device)\n","            label = label.to(device)\n","            pred = model(batch)\n","            correct += (torch.argmax(pred,dim=1)==label).sum().item()\n","    acc = correct/len(loader.dataset)\n","    print(\"Evaluation accuracy: {}\".format(acc))\n","    return acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2fP8uKFNh8h4","colab_type":"text"},"source":["# **ResNet50 with One Fully Connected Layer**"]},{"cell_type":"code","metadata":{"id":"_vEBjhU-h8HY","colab_type":"code","colab":{}},"source":["# Validation accuracy: \n","# Test accuracy: \n","model = models.resnet50(pretrained=True)\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# Parameters of newly constructed modules have requires_grad=True by default\n","model.fc = nn.Linear(model.fc.in_features, num_classes)\n","\n","model = model.to(device)\n","criterion = nn.CrossEntropyLoss(weight)\n","\n","# Only parameters of final layer are being optimized\n","optimizer_conv = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n","\n","# Decay LR by a factor of 0.7 every 5 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=5, gamma=0.7)\n","\n","# num_epoch\n","model = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25)\n","\n","acc = evaluate(model, testloader)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mx-EwZwwJUq4","colab_type":"text"},"source":["# **ResNet50 with Custom Layers**"]},{"cell_type":"code","metadata":{"id":"Ab7UhpDsJTd7","colab_type":"code","colab":{}},"source":["resnet50pre = models.resnet50(pretrained=True)\n","num_features = resnet50pre.fc.in_features\n","ofeatures = resnet50pre.fc.out_features\n","resnet50 = nn.Sequential(*list(resnet50pre.children())[:-2])\n","\n","for param in resnet50.parameters():\n","   param.requires_grad = False\n","\n","model = nn.Sequential(\n","    resnet50,\n","    nn.Conv2d(2048, 64, kernel_size=(1, 1), stride=(1, 1), bias=False),\n","    nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","    nn.Conv2d(64, 256,kernel_size=(1, 1), stride=(1, 1), bias=False),\n","    nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","    nn.Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False),\n","    nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","    nn.ReLU(),\n","    nn.AdaptiveAvgPool2d((1, 1)),\n","    nn.Flatten(),\n","    nn.Linear(num_features, num_classes)\n",")\n","\n","if torch.cuda.is_available():\n","    model.cuda()\n","\n","# Hyperparameters after random search\n","optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n","start_time = time.time()\n","train(model, trainloader, valloader, 10) \n","print('Training time: {:10f} minutes'.format((time.time()-start_time)/60))\n","\n","print(\"Evaluate on validation set...\")\n","evaluate(model, valloader)\n","print(\"Evaluate on test set\")\n","evaluate(model, testloader)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xnTm4MoLx2CM","colab_type":"text"},"source":["# **Best ResNet50 Model with Custom Classifier**"]},{"cell_type":"code","metadata":{"id":"WBCUG-lNmZyb","colab_type":"code","colab":{}},"source":["# Validation accuracy: 69.05%\n","# Test accuracy: 65.03%\n","dataloaders = {'train': trainloader, 'val': valloader}\n","dataset_sizes = {'train':  train_size, 'val': val_size}\n","\n","model_conv = models.resnet50(pretrained=True)\n","for param in model_conv.parameters():\n","    param.requires_grad = False\n","\n","# Parameters of newly constructed modules have requires_grad=True by default\n","num_ftrs = model_conv.fc.in_features\n","model_conv.fc = nn.Sequential(\n","    nn.Linear(num_ftrs, 1024), \n","    nn.ReLU(inplace=True), \n","    nn.Linear(1024, 256),\n","    nn.ReLU(inplace=True),\n","    nn.Linear(256, num_classes),\n","    nn.LogSoftmax(dim=1))\n","\n","model_conv = model_conv.to(device)\n","criterion = nn.CrossEntropyLoss(weight)\n","\n","# Hyperparameters after random search\n","optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=5, gamma=0.7)\n","model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25)\n","\n","evaluate(model_conv, valloader)\n","evaluate(model_conv, testloader)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R--H87JK1gX1","colab_type":"text"},"source":["Save Model"]},{"cell_type":"code","metadata":{"id":"desPG5JZ1e3B","colab_type":"code","colab":{}},"source":["torch.save(model_conv.state_dict(), '/content/gdrive/Shared drives/eecs442-project/resnet/saved_models/model_acc_69.05.pth')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uWtk-U1W3BSZ","colab_type":"text"},"source":["# **Visualize Model**"]},{"cell_type":"markdown","metadata":{"id":"gO6Tp8wDW-2y","colab_type":"text"},"source":["Load Model"]},{"cell_type":"code","metadata":{"id":"a6giaVANW-d6","colab_type":"code","colab":{}},"source":["best_resnet = models.resnet50(pretrained=True)\n","for param in best_resnet.parameters():\n","    param.requires_grad = False\n","\n","best_resnet.fc = nn.Sequential(\n","    nn.Linear(best_resnet.fc.in_features, 1024), \n","    nn.ReLU(inplace=True), \n","    nn.Linear(1024, 256),\n","    nn.ReLU(inplace=True),\n","    nn.Linear(256, num_classes),\n","    nn.LogSoftmax(dim=1))\n","\n","best_resnet = best_resnet.to(device)\n","best_resnet.load_state_dict(torch.load('/content/gdrive/Shared drives/eecs442-project/resnet/saved_models/model_acc_69.05.pth'))\n","\n","print(best_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHH9THQmWtnh","colab_type":"code","colab":{}},"source":["from torchvision import utils\n","\n","# this function has been adapted from the pytorch tutorial\n","# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n","def visualize_model(model, num_images=6):\n","    was_training = model.training\n","    model.eval()\n","    images_so_far = 0\n","    plt.style.use('default')\n","    fig = plt.figure(figsize=(5,6))\n","  \n","    with torch.no_grad():\n","        for i, (inputs, labels) in enumerate(dataloaders['val']):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","   \n","            for j in range(inputs.size()[0]):\n","                images_so_far += 1\n","                ax = plt.subplot(num_images//2, 2, images_so_far)\n","                ax.axis('off')\n","                pred_class_name = class_names[preds[j]]\n","                act_class_name = class_names[labels[j]]\n","                ax.set_title('Prediction: \\n {} \\n Actual: \\n {}'.format(pred_class_name, act_class_name), fontsize=\"8\")\n","                image = inputs.cpu().data[j]\n","                image -= image.min()\n","                image /= image.max()\n","                plt.tight_layout()\n","                plt.imshow(utils.make_grid(image, nrow=5).permute(1, 2, 0))\n","\n","                if images_so_far == num_images:\n","                    model.train(mode=was_training)\n","                    return\n","        model.train(mode=was_training)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ZG6HyKTWb1B","colab_type":"code","colab":{}},"source":["visualize_model(best_resnet, num_images=4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nBpacohBGx7N","colab_type":"text"},"source":["# **Load Data for InceptionV3**"]},{"cell_type":"code","metadata":{"id":"uV6qDIO0GxOe","colab_type":"code","colab":{}},"source":["# Image transformations\n","trans = {\n","    # Train uses data augmentation\n","    'train':\n","    transforms.Compose([\n","        transforms.RandomResizedCrop(size=300, scale=(0.8, 1.0)),\n","        transforms.RandomRotation(degrees=15),\n","        transforms.ColorJitter(),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.CenterCrop(size=299),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    # Validation does not use augmentation\n","    'valid':\n","    transforms.Compose([\n","        transforms.Resize(size=300),\n","        transforms.CenterCrop(size=299),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], \n","                             [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","trainloader, valloader = None, None\n","train_size, val_size = 0, 0\n","testloader = None\n","\n","num_epoch = 10\n","num_classes = 25\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Configure device\n","\n","dir = '/content/gdrive/Shared drives/eecs442-project/'\n","if num_classes == 10:\n","    train_dir = dir + 'arc10/arc10_train'\n","    test_dir = dir + '/arc10/arc10_test'\n","else: \n","    train_dir = dir + 'arc25/arc25_train'\n","    test_dir = dir + 'arc25/arc25_test'\n","\n","trainloader, valloader, train_size, val_size, mapping = LoadTrainDataSet(train_dir, 32, trans['train'], trans['valid'])\n","testloader, class_names = LoadTestDataSet(test_dir, 8, trans['valid'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oz3gXmMQH3_L","colab_type":"text"},"source":["# **Evaluate InceptionV3 models**"]},{"cell_type":"code","metadata":{"id":"Sxa-n-izHKY3","colab_type":"code","colab":{}},"source":["# The InceptionV3 models were run in a seperate notebook\n","# Below is just an example of how the InceptionV3 models were trained and evaluated\n","# This code does include all the evalution we have done on InceptionV3\n","\n","dataloaders = {'train': trainloader, 'val': valloader}\n","dataset_sizes = {'train':  train_size, 'val': val_size}\n","\n","model_conv = models.inception_v3(pretrained=True)\n","for param in model_conv.parameters():\n","    param.requires_grad = False\n","\n","# Parameters of newly constructed modules have requires_grad=True by default\n","num_ftrs = model_conv.fc.in_features\n","num_ftrs2 = model_conv.AuxLogits.fc.in_features\n","# model_conv.fc = nn.Linear(num_ftrs, num_classes)\n","model_conv.fc = nn.Sequential(\n","    nn.Linear(num_ftrs, 1024), \n","    nn.ReLU(inplace=True), \n","    nn.Linear(1024, 256),\n","    nn.ReLU(inplace=True),\n","    nn.Linear(256, num_classes),\n","    nn.LogSoftmax(dim=1))\n","model_conv.AuxLogits.fc = nn.Sequential(\n","    nn.Linear(num_ftrs2, 1024), \n","    nn.ReLU(inplace=True), \n","    nn.Linear(1024, 256),\n","    nn.ReLU(inplace=True),\n","    nn.Linear(256, num_classes),\n","    nn.LogSoftmax(dim=1))\n","\n","model_conv = model_conv.to(device)\n","criterion = nn.CrossEntropyLoss(weight)\n","\n","# Hyperparameters after random search\n","optimizer_conv = optim.SGD(list(list(model_conv.fc.parameters()) + list(model_conv.AuxLogits.parameters())), lr=0.01, momentum=0.9, weight_decay=1e-5)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=5, gamma=0.7)\n","model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=20)\n","\n","evaluate(model_conv, testloader)"],"execution_count":0,"outputs":[]}]}